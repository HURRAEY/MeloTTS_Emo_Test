# ğŸ¯ Applio â†’ MeloTTS ê°ì • íŒŒì¸íŠœë‹ ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ **ì¤€ë¹„ ì‚¬í•­**

### 1. **Applio ë°ì´í„° êµ¬ì¡° í™•ì¸**

Applioë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ë‹¤ìŒ ì¤‘ ì–´ë–¤ í˜•íƒœì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:

#### **íŒ¨í„´ A: ê°ì •ë³„ í´ë” êµ¬ì¡°**

```
applio_output/
â”œâ”€â”€ happy/
â”‚   â”œâ”€â”€ speaker1_happy_001.wav
â”‚   â”œâ”€â”€ speaker1_happy_002.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sad/
â”‚   â”œâ”€â”€ speaker1_sad_001.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ angry/
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.csv (ì„ íƒì‚¬í•­)
```

#### **íŒ¨í„´ B: í”Œë« êµ¬ì¡° + ë©”íƒ€ë°ì´í„°**

```
applio_output/
â”œâ”€â”€ wavs/
â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.csv
```

#### **íŒ¨í„´ C: í…ìŠ¤íŠ¸ íŒŒì¼ ë¦¬ìŠ¤íŠ¸**

```
applio_output/
â”œâ”€â”€ wavs/
â””â”€â”€ filelist.txt  # "íŒŒì¼ê²½ë¡œ|í…ìŠ¤íŠ¸|ê°ì •" í˜•íƒœ
```

### 2. **í•„ìš”í•œ ì •ë³´ ìˆ˜ì§‘**

ë‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# 1. Applio ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la /path/to/your/applio/output/

# 2. ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸ (ìˆë‹¤ë©´)
head -10 metadata.csv
# ë˜ëŠ”
head -10 filelist.txt

# 3. ê°ì •ë³„ íŒŒì¼ ìˆ˜ í™•ì¸
find /path/to/applio/output -name "*.wav" | wc -l
```

## ğŸ”§ **ë‹¨ê³„ë³„ ë³€í™˜ ê³¼ì •**

### **1ë‹¨ê³„: ë°ì´í„° ë³€í™˜**

```bash
# Applio ë°ì´í„°ë¥¼ MeloTTS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
python applio_to_melotts_converter.py \
    --applio_dir /path/to/your/applio/output \
    --output_dir ./emotion_dataset \
    --language KR  # ë˜ëŠ” EN, JP ë“±
```

#### **ë³€í™˜ ê²°ê³¼**

```
emotion_dataset/
â”œâ”€â”€ wavs/
â”‚   â”œâ”€â”€ happy_0001.wav
â”‚   â”œâ”€â”€ happy_0002.wav
â”‚   â”œâ”€â”€ sad_0001.wav
â”‚   â””â”€â”€ ...
â”œâ”€â”€ metadata.list
â””â”€â”€ config.json
```

### **2ë‹¨ê³„: ë°ì´í„°ì…‹ ë¶„í• **

```bash
# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  (90:10 ë¹„ìœ¨)
python split_dataset.py \
    --metadata emotion_dataset/metadata.list \
    --output_dir emotion_dataset \
    --val_ratio 0.1
```

### **3ë‹¨ê³„: MeloTTS ëª¨ë¸ ìˆ˜ì •**

#### **A. ê°ì • ì„ë² ë”© ì¶”ê°€**

`MeloTTS/melo/models.py`ì˜ `SynthesizerTrn` í´ë˜ìŠ¤ ìˆ˜ì •:

```python
class SynthesizerTrn(nn.Module):
    def __init__(self, ..., n_emotions=7, **kwargs):
        super().__init__()
        # ... ê¸°ì¡´ ì½”ë“œ ...

        # ê°ì • ì„ë² ë”© ì¶”ê°€
        self.n_emotions = n_emotions
        if n_emotions > 0:
            self.emb_emotions = nn.Embedding(n_emotions, gin_channels)

    def forward(self, x, x_lengths, y, y_lengths, sid, tone, language,
                bert, ja_bert, emotion_id=None):

        # ìŠ¤í”¼ì»¤ ì„ë² ë”©
        if self.n_speakers > 0:
            g_spk = self.emb_g(sid).unsqueeze(-1)
        else:
            g_spk = self.ref_enc(y.transpose(1, 2)).unsqueeze(-1)

        # ê°ì • ì„ë² ë”© ì¶”ê°€
        if emotion_id is not None and self.n_emotions > 0:
            g_emo = self.emb_emotions(emotion_id).unsqueeze(-1)
            g = g_spk + g_emo  # ìŠ¤í”¼ì»¤ + ê°ì • ê²°í•©
        else:
            g = g_spk

        # ... ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼ ...
```

#### **B. ë°ì´í„° ë¡œë” ìˆ˜ì •**

`MeloTTS/melo/data_utils.py` ìˆ˜ì •:

```python
def get_audio_text_speaker_pair(self, audiopath_sid_text):
    # ê°ì • ì •ë³´ ì¶”ê°€ íŒŒì‹±
    if len(audiopath_sid_text) >= 8:  # ê°ì • ì •ë³´ í¬í•¨
        audiopath, sid, language, text, phones, tone, word2ph, emotion_id = audiopath_sid_text
        emotion_id = torch.LongTensor([int(emotion_id)])
    else:  # ê¸°ì¡´ í˜•íƒœ
        audiopath, sid, language, text, phones, tone, word2ph = audiopath_sid_text
        emotion_id = torch.LongTensor([0])  # neutral

    # ... ê¸°ì¡´ ì²˜ë¦¬ ...

    return (phones, spec, wav, sid, tone, language, bert, ja_bert, emotion_id)
```

### **4ë‹¨ê³„: ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸**

#### **emotion_config.json ìƒì„±**

```json
{
  "train": {
    "log_interval": 200,
    "eval_interval": 1000,
    "seed": 52,
    "epochs": 10000,
    "learning_rate": 0.0002,
    "betas": [0.8, 0.99],
    "eps": 1e-9,
    "batch_size": 4,
    "fp16_run": false,
    "lr_decay": 0.999875,
    "segment_size": 16384,
    "init_lr_ratio": 1,
    "warmup_epochs": 0,
    "c_mel": 45,
    "c_kl": 1.0,
    "skip_optimizer": true
  },
  "data": {
    "training_files": "emotion_dataset/train.list",
    "validation_files": "emotion_dataset/val.list",
    "max_wav_value": 32768.0,
    "sampling_rate": 44100,
    "filter_length": 2048,
    "hop_length": 512,
    "win_length": 2048,
    "n_mel_channels": 128,
    "mel_fmin": 0.0,
    "mel_fmax": null,
    "add_blank": true,
    "n_speakers": 1,
    "n_emotions": 7,
    "cleaned_text": true,
    "spk2id": { "speaker_00": 0 },
    "emotion_map": {
      "neutral": 0,
      "happy": 1,
      "sad": 2,
      "angry": 3,
      "surprised": 4,
      "fearful": 5,
      "disgusted": 6
    }
  },
  "model": {
    "use_spk_conditioned_encoder": true,
    "use_noise_scaled_mas": true,
    "use_mel_posterior_encoder": false,
    "use_duration_discriminator": true,
    "inter_channels": 192,
    "hidden_channels": 192,
    "filter_channels": 768,
    "n_heads": 2,
    "n_layers": 6,
    "n_layers_trans_flow": 3,
    "kernel_size": 3,
    "p_dropout": 0.1,
    "resblock": "1",
    "resblock_kernel_sizes": [3, 7, 11],
    "resblock_dilation_sizes": [
      [1, 3, 5],
      [1, 3, 5],
      [1, 3, 5]
    ],
    "upsample_rates": [8, 8, 2, 2, 2],
    "upsample_initial_channel": 512,
    "upsample_kernel_sizes": [16, 16, 8, 2, 2],
    "n_layers_q": 3,
    "use_spectral_norm": false,
    "gin_channels": 256,
    "n_emotions": 7
  }
}
```

### **5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**

```bash
cd MeloTTS/melo
python preprocess_text.py \
    --metadata ../../emotion_dataset/train.list \
    --config_path emotion_config.json \
    --val-per-spk 10 \
    --max-val-total 100
```

### **6ë‹¨ê³„: ê°ì • ëª¨ë¸ í›ˆë ¨**

```bash
# ë‹¨ì¼ GPU
python train.py --config emotion_config.json

# ë‹¤ì¤‘ GPU (ê¶Œì¥)
torchrun --nproc_per_node=2 train.py --config emotion_config.json
```

## ğŸ¯ **ì¶”ë¡  ë° ì‚¬ìš©**

### **ê°ì •ë³„ ìŒì„± í•©ì„±**

```python
import torch
from melo.api import TTS

# ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model = TTS(language='KR', device=device)
model.load_checkpoint('path/to/emotion_model.pth')

# ê°ì •ë³„ í•©ì„±
emotions = {
    'neutral': 0, 'happy': 1, 'sad': 2, 'angry': 3,
    'surprised': 4, 'fearful': 5, 'disgusted': 6
}

text = "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”!"

for emotion_name, emotion_id in emotions.items():
    audio = model.tts_to_file(
        text=text,
        speaker_id=0,
        emotion_id=emotion_id,
        output_path=f"output_{emotion_name}.wav",
        speed=1.0,
        noise_scale=0.667,
        noise_scale_w=0.8
    )
    print(f"âœ… {emotion_name} ìŒì„± ìƒì„± ì™„ë£Œ")
```

## ğŸ“Š **ë°ì´í„° ìš”êµ¬ì‚¬í•­**

### **ìµœì†Œ ìš”êµ¬ì‚¬í•­**

- ê°ì •ë‹¹ **ìµœì†Œ 30ë¶„** (ì•½ 600 utterances)
- ì´ **3.5ì‹œê°„** (7ê°œ ê°ì •)

### **ê¶Œì¥ ìš”êµ¬ì‚¬í•­**

- ê°ì •ë‹¹ **2ì‹œê°„** (ì•½ 2400 utterances)
- ì´ **14ì‹œê°„** (7ê°œ ê°ì •)

### **ê³ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­**

- ê°ì •ë‹¹ **5ì‹œê°„+** (6000+ utterances)
- ì´ **35ì‹œê°„+** (7ê°œ ê°ì •)

## ğŸ”§ **ë¬¸ì œ í•´ê²°**

### **Q1: Applio íŒŒì¼ êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš°**

```python
# applio_to_melotts_converter.pyì˜ _extract_emotion_from_path í•¨ìˆ˜ ìˆ˜ì •
def _extract_emotion_from_path(self, file_path):
    path_lower = file_path.lower()

    # ì‚¬ìš©ì ë§ì¶¤ íŒ¨í„´ ì¶”ê°€
    emotion_patterns = {
        'neutral': ['neutral', 'normal', 'ì¤‘ì„±'],
        'happy': ['happy', 'joy', 'í–‰ë³µ', 'ê¸°ì¨'],
        'sad': ['sad', 'sorrow', 'ìŠ¬í””', 'ìš°ìš¸'],
        'angry': ['angry', 'mad', 'í™”ë‚¨', 'ë¶„ë…¸'],
        # ... ì¶”ê°€ íŒ¨í„´
    }

    for emotion, patterns in emotion_patterns.items():
        if any(pattern in path_lower for pattern in patterns):
            return emotion

    return 'neutral'
```

### **Q2: ë©”íƒ€ë°ì´í„° í˜•ì‹ì´ ë‹¤ë¥¸ ê²½ìš°**

```python
# CSV ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¸ ê²½ìš° ë§¤í•‘
COLUMN_MAPPING = {
    'file_name': 'file_path',
    'transcript': 'text',
    'emotion_label': 'emotion',
    'speaker_name': 'speaker_id'
}
```

### **Q3: í›ˆë ¨ ì¤‘ ë©”ëª¨ë¦¬ ë¶€ì¡±**

```json
// emotion_config.jsonì—ì„œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
{
  "train": {
    "batch_size": 2, // 4ì—ì„œ 2ë¡œ ê°ì†Œ
    "segment_size": 8192 // 16384ì—ì„œ 8192ë¡œ ê°ì†Œ
  }
}
```

## ğŸ“ˆ **ì„±ëŠ¥ í‰ê°€**

### **ê°ê´€ì  í‰ê°€**

```python
# ê°ì • ì¸ì‹ ì •í™•ë„ ì¸¡ì •
from emotion2vec import Emotion2Vec

evaluator = Emotion2Vec.from_pretrained("emotion2vec/emotion2vec_plus_large")

def evaluate_emotion_accuracy(generated_audios, target_emotions):
    correct = 0
    total = len(generated_audios)

    for audio, target in zip(generated_audios, target_emotions):
        predicted = evaluator.predict(audio)
        if predicted == target:
            correct += 1

    accuracy = correct / total
    print(f"ê°ì • ì¸ì‹ ì •í™•ë„: {accuracy:.2%}")
    return accuracy
```

### **ì£¼ê´€ì  í‰ê°€**

```python
# MOS (Mean Opinion Score) í‰ê°€
def collect_mos_scores():
    emotions = ['neutral', 'happy', 'sad', 'angry']

    for emotion in emotions:
        print(f"\n{emotion} ìŒì„± í’ˆì§ˆ í‰ê°€ (1-5ì ):")
        # í‰ê°€ìë“¤ì—ê²Œ ìŒì„± ì¬ìƒ í›„ ì ìˆ˜ ìˆ˜ì§‘
        pass
```

ì´ì œ **Applio ë°ì´í„°ì˜ êµ¬ì²´ì ì¸ êµ¬ì¡°**ë¥¼ ì•Œë ¤ì£¼ì‹œë©´, ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í•´ë‹¹ í˜•íƒœì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
