#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TDM_LLJ (Applio RVC ì „ì²˜ë¦¬) ë°ì´í„°ë¥¼ MeloTTS í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import shutil
import librosa
import soundfile as sf
from pathlib import Path
from pydub import AudioSegment
from pydub.silence import split_on_silence
import argparse

# ê°ì • ë§¤í•‘ (í•œêµ­ì–´ â†’ ì˜ì–´ â†’ ID)
EMOTION_MAP = {
    # í•œêµ­ì–´
    'ê¸°ì¨': 'happy',
    'ìŠ¬í””': 'sad', 
    'í™”ë‚¨': 'angry',
    'ìš°ìš¸': 'depressed',
    'ì¤‘ì„±': 'neutral',
    # ì˜ì–´ (ì´ë¯¸ ìˆëŠ” ê²½ìš°)
    'happy': 'happy',
    'sad': 'sad',
    'angry': 'angry',
    'joy': 'happy',
    'depression': 'depressed',
    'neutral': 'neutral'
}

EMOTION_ID_MAP = {
    'neutral': 0,
    'happy': 1,
    'sad': 2,
    'angry': 3,
    'depressed': 4,
    'surprised': 5,
    'fearful': 6
}

class TDMToMeloTTSConverter:
    def __init__(self, tdm_dir, output_dir, language='KR', segment_length=10):
        """
        Args:
            tdm_dir: TDM_LLJ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            language: ì–¸ì–´ ì½”ë“œ (KR, EN, JP)
            segment_length: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        """
        self.tdm_dir = Path(tdm_dir)
        self.output_dir = Path(output_dir)
        self.language = language
        self.segment_length = segment_length
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "wavs").mkdir(exist_ok=True)
        
    def convert_tdm_to_melotts(self):
        """TDM ë°ì´í„°ë¥¼ MeloTTS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        print("ğŸµ TDM_LLJ â†’ MeloTTS ë°ì´í„° ë³€í™˜ ì‹œì‘...")
        
        melotts_data = []
        file_counter = 0
        
        # ì–¸ì–´ë³„ í´ë” íƒìƒ‰
        language_folders = []
        
        # ì–¸ì–´ í´ë” ì°¾ê¸°
        for folder in self.tdm_dir.iterdir():
            if folder.is_dir():
                folder_name = folder.name.lower()
                if any(lang in folder_name for lang in ['ì˜ì–´', 'ì¼ì–´', 'í•œêµ­ì–´', 'en', 'jp', 'kr']):
                    language_folders.append(folder)
                    print(f"ğŸ“ ì–¸ì–´ í´ë” ë°œê²¬: {folder.name}")
        
        # ì–¸ì–´ í´ë”ê°€ ì—†ìœ¼ë©´ ë£¨íŠ¸ì—ì„œ ì§ì ‘ ê°ì • í´ë” ì°¾ê¸°
        if not language_folders:
            language_folders = [self.tdm_dir]
        
        for lang_folder in language_folders:
            print(f"\nğŸ” ì²˜ë¦¬ ì¤‘: {lang_folder.name}")
            
            # ê°ì • í´ë” ì°¾ê¸°
            emotion_folders = self._find_emotion_folders(lang_folder)
            
            for emotion_folder in emotion_folders:
                emotion_name = self._extract_emotion_from_folder(emotion_folder.name)
                emotion_id = EMOTION_ID_MAP.get(emotion_name, 0)
                
                print(f"  ğŸ˜Š ê°ì • ì²˜ë¦¬: {emotion_folder.name} â†’ {emotion_name} (ID: {emotion_id})")
                
                # PTD í´ë”ì—ì„œ WAV íŒŒì¼ ì°¾ê¸°
                ptd_folder = emotion_folder / "PTD"
                if ptd_folder.exists():
                    wav_files = list(ptd_folder.glob("*.wav"))
                else:
                    wav_files = list(emotion_folder.glob("*.wav"))
                
                print(f"    ğŸµ WAV íŒŒì¼ {len(wav_files)}ê°œ ë°œê²¬")
                
                for wav_file in wav_files:
                    segments = self._split_audio_file(wav_file)
                    
                    for i, segment_audio in enumerate(segments):
                        # ìƒˆ íŒŒì¼ëª… ìƒì„±
                        new_filename = f"{emotion_name}_{file_counter:04d}.wav"
                        new_path = self.output_dir / "wavs" / new_filename
                        
                        # ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
                        sf.write(new_path, segment_audio, 22050)
                        
                        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ASRì´ë‚˜ ìˆ˜ë™ ë¼ë²¨ë§ í•„ìš”)
                        text = self._generate_sample_text(emotion_name, file_counter)
                        
                        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        melotts_data.append({
                            'wav_path': f"wavs/{new_filename}",
                            'speaker_id': 'speaker_00',
                            'language': self.language,
                            'text': text,
                            'emotion_id': emotion_id,
                            'emotion_name': emotion_name,
                            'original_file': str(wav_file)
                        })
                        
                        file_counter += 1
                        
                        if file_counter % 100 == 0:
                            print(f"    âœ… {file_counter}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        
        return self._save_melotts_format(melotts_data)
    
    def _find_emotion_folders(self, parent_folder):
        """ê°ì • í´ë”ë“¤ì„ ì°¾ê¸°"""
        emotion_folders = []
        
        for folder in parent_folder.iterdir():
            if folder.is_dir():
                folder_name = folder.name
                # ê°ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ í´ë” ì°¾ê¸°
                if any(emotion in folder_name for emotion in EMOTION_MAP.keys()):
                    emotion_folders.append(folder)
        
        return emotion_folders
    
    def _extract_emotion_from_folder(self, folder_name):
        """í´ë”ëª…ì—ì„œ ê°ì • ì¶”ì¶œ"""
        folder_lower = folder_name.lower()
        
        # í•œêµ­ì–´ ê°ì •ëª… ë§¤í•‘
        for korean_emotion, english_emotion in EMOTION_MAP.items():
            if korean_emotion in folder_name or korean_emotion.lower() in folder_lower:
                return english_emotion
        
        # ê¸°ë³¸ê°’
        return 'neutral'
    
    def _split_audio_file(self, wav_file):
        """ê¸´ ìŒì„± íŒŒì¼ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• """
        try:
            print(f"      ğŸ”ª ë¶„í•  ì¤‘: {wav_file.name}")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(wav_file, sr=22050)
            
            # ë¬´ìŒ ê¸°ë°˜ ë¶„í• 
            audio_segment = AudioSegment.from_wav(wav_file)
            
            # ë¬´ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ìµœì†Œ ê¸¸ì´: 3ì´ˆ, ìµœëŒ€ ê¸¸ì´: 15ì´ˆ)
            chunks = split_on_silence(
                audio_segment,
                min_silence_len=1000,  # 1ì´ˆ ë¬´ìŒ
                silence_thresh=-40,     # -40dB ì´í•˜ë¥¼ ë¬´ìŒìœ¼ë¡œ ê°„ì£¼
                keep_silence=500       # ì•ë’¤ 0.5ì´ˆ ë¬´ìŒ ìœ ì§€
            )
            
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§
            filtered_chunks = []
            for chunk in chunks:
                duration = len(chunk) / 1000.0  # ì´ˆ ë‹¨ìœ„
                if 3.0 <= duration <= 15.0:  # 3-15ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ìœ ì§€
                    filtered_chunks.append(chunk)
            
            # AudioSegmentë¥¼ numpy arrayë¡œ ë³€í™˜
            segments = []
            for chunk in filtered_chunks[:50]:  # ìµœëŒ€ 50ê°œ ì„¸ê·¸ë¨¼íŠ¸
                # AudioSegmentë¥¼ WAV ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ ë¡œë“œ
                chunk_audio = chunk.set_frame_rate(22050).set_channels(1)
                audio_array = chunk_audio.get_array_of_samples()
                audio_np = librosa.util.buf_to_float(audio_array, n_bytes=2)
                segments.append(audio_np)
            
            print(f"        âœ‚ï¸  {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±")
            return segments
            
        except Exception as e:
            print(f"        âŒ ë¶„í•  ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_sample_text(self, emotion_name, counter):
        """ê°ì •ì— ë§ëŠ” ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        text_templates = {
            'happy': [
                "ì˜¤ëŠ˜ ì •ë§ ê¸°ìœ í•˜ë£¨ì˜€ì–´ìš”!",
                "ì™€, ì´ê±° ì •ë§ ëŒ€ë‹¨í•˜ë„¤ìš”!",
                "í–‰ë³µí•œ ìˆœê°„ì´ë„¤ìš”.",
                "ì •ë§ ì¦ê±°ì›Œìš”!",
                "ê¸°ì¨ì´ ë„˜ì³ë‚˜ë„¤ìš”."
            ],
            'sad': [
                "ì •ë§ ìŠ¬í”ˆ ì¼ì´ì—ìš”.",
                "ë§ˆìŒì´ ì•„íŒŒìš”.",
                "ëˆˆë¬¼ì´ ë‚˜ë„¤ìš”.",
                "ìŠ¬í”ˆ í•˜ë£¨ì˜€ì–´ìš”.",
                "ê°€ìŠ´ì´ ë¨¹ë¨¹í•´ìš”."
            ],
            'angry': [
                "ì •ë§ í™”ê°€ ë‚˜ë„¤ìš”!",
                "ì´ê±´ ë„ˆë¬´í•´ìš”!",
                "ì°¸ì„ ìˆ˜ ì—†ì–´ìš”!",
                "í™”ê°€ ì¹˜ë°€ì–´ìš”!",
                "ë¶„ë…¸ê°€ ì†Ÿêµ¬ì³ìš”!"
            ],
            'depressed': [
                "ìš°ìš¸í•œ ê¸°ë¶„ì´ì—ìš”.",
                "í˜ë“  í•˜ë£¨ë„¤ìš”.",
                "ê¸°ìš´ì´ ì—†ì–´ìš”.",
                "ì¹¨ìš¸í•œ ë§ˆìŒì´ì—ìš”.",
                "ìš°ìš¸ì¦ì´ ëŠê»´ì ¸ìš”."
            ],
            'neutral': [
                "ì•ˆë…•í•˜ì„¸ìš”.",
                "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
                "ì¼ë°˜ì ì¸ ëŒ€í™”ì…ë‹ˆë‹¤.",
                "í‰ë²”í•œ í•˜ë£¨ì…ë‹ˆë‹¤.",
                "ì¤‘ì„±ì ì¸ í†¤ì´ì—ìš”."
            ]
        }
        
        templates = text_templates.get(emotion_name, text_templates['neutral'])
        return templates[counter % len(templates)]
    
    def _save_melotts_format(self, melotts_data):
        """MeloTTS í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        
        if not melotts_data:
            print("âŒ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return False
        
        print(f"\nğŸ’¾ {len(melotts_data)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ MeloTTS í˜•ì‹ìœ¼ë¡œ ì €ì¥ì¤‘...")
        
        # 1. metadata.list ìƒì„±
        metadata_lines = []
        for data in melotts_data:
            line = f"{data['wav_path']}|{data['speaker_id']}|{data['language']}|{data['text']}|{data['emotion_id']}"
            metadata_lines.append(line)
        
        metadata_file = self.output_dir / "metadata.list"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(metadata_lines))
        
        # 2. ê°ì •ë³„ í†µê³„
        emotion_stats = {}
        for data in melotts_data:
            emotion_name = data['emotion_name']
            emotion_stats[emotion_name] = emotion_stats.get(emotion_name, 0) + 1
        
        # 3. config.json ìƒì„±
        config = {
            "data": {
                "training_files": "train.list",
                "validation_files": "val.list",
                "n_emotions": len(EMOTION_ID_MAP),
                "emotion_map": EMOTION_ID_MAP,
                "language": self.language,
                "segment_length": self.segment_length
            },
            "model": {
                "n_emotions": len(EMOTION_ID_MAP),
                "emotion_channels": 256
            }
        }
        
        config_file = self.output_dir / "config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # 4. ë³€í™˜ ë¡œê·¸ ì €ì¥
        log_data = {
            "total_segments": len(melotts_data),
            "emotion_distribution": emotion_stats,
            "source_files": list(set(data['original_file'] for data in melotts_data))
        }
        
        log_file = self.output_dir / "conversion_log.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print("âœ… TDM â†’ MeloTTS ë³€í™˜ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(melotts_data)}ê°œ")
        print("ğŸ“ˆ ê°ì •ë³„ ë¶„í¬:")
        
        for emotion, count in sorted(emotion_stats.items()):
            print(f"   {emotion}: {count}ê°œ ({count/len(melotts_data)*100:.1f}%)")
        
        print(f"\nğŸ“„ ìƒì„±ëœ íŒŒì¼:")
        print(f"   - metadata.list: ë©”íƒ€ë°ì´í„°")
        print(f"   - config.json: ì„¤ì • íŒŒì¼")
        print(f"   - conversion_log.json: ë³€í™˜ ë¡œê·¸")
        print(f"   - wavs/: ì„¸ê·¸ë¨¼íŠ¸ ìŒì„± íŒŒì¼ë“¤")
        
        return True

def main():
    parser = argparse.ArgumentParser(description="TDM_LLJ ë°ì´í„°ë¥¼ MeloTTS í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
    parser.add_argument("--tdm_dir", default="MeloTTS/TDM_LLJ", help="TDM_LLJ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output_dir", default="./emotion_dataset_tdm", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--language", default="KR", help="ì–¸ì–´ ì½”ë“œ (KR, EN, JP)")
    parser.add_argument("--segment_length", type=int, default=10, help="ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)")
    
    args = parser.parse_args()
    
    converter = TDMToMeloTTSConverter(
        tdm_dir=args.tdm_dir,
        output_dir=args.output_dir,
        language=args.language,
        segment_length=args.segment_length
    )
    
    success = converter.convert_tdm_to_melotts()
    
    if success:
        print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ë°ì´í„° ë¶„í• : python split_dataset.py --metadata emotion_dataset_tdm/metadata.list --output_dir emotion_dataset_tdm")
        print("2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: cd MeloTTS/melo && python preprocess_text.py --metadata ../../emotion_dataset_tdm/train.list")
        print("3. ê°ì • ëª¨ë¸ í›ˆë ¨: python train.py --config ../../emotion_dataset_tdm/config.json")
        print("\nğŸ’¡ ì°¸ê³ : ìƒì„±ëœ í…ìŠ¤íŠ¸ëŠ” ìƒ˜í”Œì´ë¯€ë¡œ, ì‹¤ì œ ì „ì‚¬(transcription)ë¡œ êµì²´í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        print("âŒ ë³€í™˜ ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 